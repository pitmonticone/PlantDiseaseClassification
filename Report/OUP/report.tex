\documentclass[12pt,halfline,a4paper,]{ouparticle}

% Packages I think are necessary for basic Rmarkdown functionality
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{fancyvrb}
\usepackage{framed}

%% To allow better options for figure placement
%\usepackage{float}

% Packages that are supposedly required by OUP sty file
\usepackage{amssymb, amsmath, geometry, amsfonts, verbatim, endnotes, setspace}

% For code highlighting I think
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

% For making Rmarkdown lists
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


% Part for setting citation format package: natbib
\usepackage{natbib}
\bibliographystyle{plainnat}

% Part for setting citation format package: biblatex

% Pandoc header

\begin{document}

\title{Convolutional Neural Network for Plant Disease Classification}

\author{%
\name{Claudio Moroni}\address{University of Turin}\email{\href{mailto:claudio.moroni@edu.unito.it}{claudio.moroni@edu.unito.it}}
\and
\name{Davide Orsenigo}\address{University of Turin}\email{\href{mailto:davide.orsenigo@edu.unito.it}{davide.orsenigo@edu.unito.it}}
\and
\name{Pietro Monticone}\address{University of Turin}\email{\href{mailto:pietro.monticone@edu.unito.it}{pietro.monticone@edu.unito.it}}
}

\abstract{In this effort we train a CNN model in order to take part in the
\href{https://www.kaggle.com/c/plant-pathology-2020-fgvc7/discussion/141015}{Plant
Pathology 2020 - FGVC7}, an image classification task where we
ultimately achieved a categorical roc score of 0.956 using
\href{https://keras.io/api/applications/densenet/\#densenet121-function}{denseNet121},
and 0.915 using a relatively shallow CNN defined and trained from
scratch. The train and the test datasets are both composed of 1821
images, showing various leaves which are to be classified in 4
categories: ``healthy'', ``multiple disases'', ``rust'', ``scab'', where
we founfd no indication that ``multiple\_diseases'' refers to both
``rust'' and ``scab'', it just seems to indicate the presece of other
(different) kinds of illnesses. The metric used is the categorical roc.
In the end, we also output a visulization of the filters and activation
maps of the layers, and an SVD decomposition of the dataset. The
techniques we used during training are balancing classes with SMOTE,
data augemntation with Keras ImageDataGenerator, optimal dropout and
epoch grid searching. Where possible, also auxiliary elements of the
pipeline (e.g.~SMOTE) have been manually fine tuned.}

\date{16 June 2020}

\keywords{deep learning; convolutional neural network; image classification}

\maketitle



\hypertarget{model-training}{%
\section{Model training}\label{model-training}}

The Challenge consisted in classifying leaves images into four
categories: HEALTHY, MULTIPLE\_DISEASES, RUST, SCAB. Altough a
MULTIPLE\_DISEASES leaf could be affected both by rust and scab, or by
rust and another disease or by scab and another disease, because there
is no taxonomy we treated the classes as mutually exclusive. This is to
say that in principle the model should distinguish between all four
classes, as none of them is an abstraction of (some of) the others. The
evaluation metric is the \protect\hyperlink{roc}{column-wise ROC}. We
implemented an explicit keras model (EKM in the following), then also a
pre-teined model - DenseNet121 - was used. In order to build a more
robust model, we tried/implemented the following techniques (in
chronological order):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Class balancing with SMOTE.
\item
  Data augmentation with keras ImageDataGenerator.
\item
  Some fine-tuning/exploration of the models' layers and parameters, in
  particular e dropout layer (EKM only) , Early Stopping (DenseNet121
  only), learning rate scheduling ( DenseNet121 only).
\item
  An attempt to put a convolutional autoencoder autoencoder between
  point 2 and point 3.
\end{enumerate}

Secondary information may be found in the
\protect\hyperlink{appendix}{Appendix}.

\hypertarget{class-balancing-with-smote}{%
\section{Class balancing with SMOTE}\label{class-balancing-with-smote}}

\texttt{SMOTE(sampling\_strategy,k\_neighbors)} is a class balancing
algorithm that operates as follows: (one of) the minority class(es) is
considered, a random point from it is picked and its first
\texttt{n\_neighbors} nearest neighbors are found. One of the latters is
then randomly selected, and the the vector between this point and the
originally selected point is drawn. This vector gets then multiplied by
a number between 0 and 1, and the resulting synthetic point is added to
the dataset. There exist many variants of \textbf{SMOTE}, so besides the
standard one also the \textbf{SVMSMOTE} and \textbf{ADASYN} have been
tried. \textbf{SVMSMOTE} is a variant of \textbf{SMOTE} that first of
all fits an SVM on the data, and uses its support vectors to identify
points more prone to misclassification (i.e.~those on the border of the
class cluster): these points are later oversampled more than the others.
\textbf{ADASYN} instead draws from a distribution over the minority
class(es) that is pointwise inversely proportional to their density.
That is, more points are generated where the minority class(es) are
sparser, and less points where they are more dense. Anyway, the class
balancing algorithm that ultimately performed better is baseline
\textbf{SMOTE}, with some fine tuning on the
\texttt{sampling\_strategy}( the \texttt{all} value means that all
classes are resampled to match the size of the majority class), and the
\texttt{n\_neighbors} parameters. See \href{}{Platform limitations}

\hypertarget{data-augmentation-with-keras-imagedatagenerator}{%
\section{Data augmentation with Keras'
ImageDataGenerator}\label{data-augmentation-with-keras-imagedatagenerator}}

Click the following link for an introduction to
\href{https://keras.io/api/preprocessing/image/}{keras Image
preprocessing API}. Using Keras' ImageDataGenerator, after a manual
inspection of the images, we founfd that the best data augmentin
technique consisted in a random planar rotation, mixed with random
horizontal flip.

\hypertarget{some-fine-tuningexploration-of-the-models-layers-and-parameters-in-particular-e-dropout-layer-ekm-only-early-stopping-densenet121-only-learning-rate-scheduling-densenet121-only-and-optimizer-variations-ekm}{%
\section{Some fine-tuning/exploration of the models' layers and
parameters, in particular e dropout layer (EKM only) , Early Stopping
(DenseNet121 only), learning rate scheduling ( DenseNet121 only) and
optimizer variations
(EKM)}\label{some-fine-tuningexploration-of-the-models-layers-and-parameters-in-particular-e-dropout-layer-ekm-only-early-stopping-densenet121-only-learning-rate-scheduling-densenet121-only-and-optimizer-variations-ekm}}

The explorations reported in the title of this section have been
performed. We couldn't implement Early Stopping in the EKM model, as
fluctuation in either validation loss, categorical accuracy and row-wose
ROC where too high to set proper \texttt{min\_delta} and
\texttt{patience} parameters in
\href{https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping}{TensorFlow's
Early Stopping} implementation. (We also explored some optimizers)
\emph{(Visto che l'early stopping di tensorflow non va, usiamo quello
manuale gi√† implementato? Quali altri optimizers abbiamo provato?
Abbiamo una submission per questi?)}. The manual implementations of the
dropout and early stopping acted simultaneoulsy, so they performed like
a grid search. The dropout and epoch values corresponding to the best
column-wise ROC were saved and used during testing phase.

\hypertarget{an-attempt-to-put-a-convolutional-autoencoder-autoencoder-between-point-2-and-point-3.}{%
\section{An attempt to put a convolutional autoencoder autoencoder
between point 2 and point
3.}\label{an-attempt-to-put-a-convolutional-autoencoder-autoencoder-between-point-2-and-point-3.}}

Despite the multiple configurations tried, the best we could get is a
\(0.7\) column-wise ROC. The reason behind it could be the fact that on
one hand an autoencoder with no pooling on the encoder side makes little
sense in terms of dimensionality reduction,s on the other hand even a
single bidimensonal maxpooling caused the output image to be too little
for last EKM layer to classify. See
\protect\hyperlink{limitations}{Platform limitations}.

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{roc}{%
\subsection{column-wise ROC}\label{roc}}

From
\href{https://www.kaggle.com/c/plant-pathology-2020-fgvc7/overview/evaluation}{Challenge
Overview} on Kaggle: \textgreater{}Submissions are evaluated on mean
column-wise ROC AUC. In other words, the score is the average of the
individual AUCs of each predicted column

\hypertarget{limitations}{%
\subsection{Platform Limitations}\label{limitations}}

The newest unstable version of TensorFlow with GPU support is needed to
run the code. Unfortunately, we haven't been able to set proper kernels
up on our local machines, so we had to rely on publicly available cloud
interactive environments like \href{https://www.kaggle.com/}{Kaggle},
that provided free out of the box kernels for our purposes. The only
limitations are in terms of cpu RAM, which forced us to downsize the
images to about \(200*200\) pixels.

\hypertarget{visualization}{%
\subsection{Visualization}\label{visualization}}

\hypertarget{pca}{%
\subsubsection{PCA}\label{pca}}

\hypertarget{convolutional-filters-and-features-maps}{%
\subsubsection{Convolutional Filters and Features
Maps}\label{convolutional-filters-and-features-maps}}




\renewcommand\refname{References}

\bibliography{biblio.bib}



\end{document}

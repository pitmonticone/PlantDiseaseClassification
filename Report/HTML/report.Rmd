---
title: "  Convolutional Neural Network for Plant Disease Classification "
subtitle: "Recognition of Foliar Diseases in Apple Trees"
author: "**Claudio Moroni** | **Davide Orsenigo** | **Pietro Monticone**"
date: "`r format(Sys.time(), '%d %B %Y')`, *University of Turin*"
output: 
 prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
fontsize: 10pt
#bibliography: bibliography.bibtex # https://doi2bib.org
#biblio-style: apalike
#link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
library(knitr)

setwd("~/github/NeuralNetworksProject/Report/HTML") # Pietro Monticone 
#setwd("~/GitHub/NeuralNetworksProject/Report/HTML") # Claudio Moroni  
#setwd("Q:\\tooBigToDrive\\plantsvillage\\Report\\report") # Claudio Moroni  
```

1. Remember to cite the Dataset Paper;
1. Separate official from attempted solutions;
1. Stress the difference between `SMOTE` and `ImageDataGenerator`; 
1. Model architecture diagram with `tensorflow`;

# **Abstract** {.unnumbered}

In this effort we train a CNN model in order to take part in the [Plant Pathology 2020 - FGVC7](https://www.kaggle.com/c/plant-pathology-2020-fgvc7/discussion/141015), an image classification task where we ultimately achieved a categorical roc score of 0.972 using [denseNet121](https://keras.io/api/applications/densenet/#densenet121-function), and 0.937 using a relatively shallow CNN defined and trained from scratch. The train and the test datasets are both composed of 1821 images, showing various leaves which are to be classified in 4 categories: "healthy", "multiple disases", "rust", "scab", where we founfd no indication that "multiple_diseases" refers to both "rust" and "scab", it just seems to indicate the presece of other (different) kinds of illnesses. The metric used is the categorical roc. In the end, we also output a visulization of the filters and activation maps of the layers, and an SVD decomposition of the dataset. The techniques we used during training are balancing classes with SMOTE, data augemntation with Keras ImageDataGenerator, optimal dropout and epoch grid searching. Where possible, also auxiliary elements of the pipeline (e.g. SMOTE) have been manually fine tuned.

The evaluation metric is the [column-averaged ROC](#roc).<br>


# **Data**

Both the training and the test [datasets](https://www.kaggle.com/c/plant-pathology-2020-fgvc7/data) are composed of 1821 high-quality, real-life symptom images of multiple apple foliar diseases, with variable illumination, angles, surfaces, and noise have been manually captured, expert-annotated to create a pilot dataset for apple scab, cedar apple rust, and healthy leaves. 


![Input Images](Images/InputImages.png)

The Challenge consisted in classifying leaves images into four categories: HEALTHY ($h$), MULTIPLE_DISEASES ($m$), RUST ($r$), SCAB ($s$). Altough a MULTIPLE_DISEASES leaf could be affected both by rust and scab, or by rust and another disease or by scab and another disease, because there is no taxonomy we treated the classes as mutually exclusive. This is to say that in principle the model should distinguish between all four classes, as none of them is an abstraction of (some of) the others. Il dataset non risulta bilanciato, bensì distribuito nel modo seguente $(h = 516, m = 91, r = 622, s = 592)$. 

Di seguito andiamo a rappresentare le prime due componenti di una [Truncated SVD](#pca) per identificare qualitativamente se le classi distinte sono clusterizzate. Come è ragionevole pensare per elementi così complessi ciò non risulta immediatamente possibile. Si può tuttavia apprezzare l'amplificazioen classi di `SMOTE` e verificare che i punti generati clusterizzano come prevedibile.

![**Figure 2.** Truncated SVD](Images/TruncatedSVD.png)


# **Class Balancing with `SMOTE`**

`SMOTE(sampling_strategy,k_neighbors)` is a class balancing algorithm that operates as follows: (one of) the minority class(es) is considered, a random point from it is picked and its first `n_neighbors` nearest neighbors are found. One of the latters is then randomly selected, and the the vector between this point and the originally selected point is drawn. This vector gets then multiplied by a number between 0 and 1, and the resulting synthetic point is added to the dataset. <br>
There exist many variants of **SMOTE**, so besides the standard one also the **SVMSMOTE** and **ADASYN** have been tried. <br>
**SVMSMOTE** is a variant of **SMOTE** that first of all fits an SVM on the data, and uses its support vectors to identify points more prone to misclassification (i.e. those on the border of the class cluster): these points are later oversampled more than the others.<br>
**ADASYN** instead draws from a distribution over the minority class(es) that is pointwise inversely proportional to their density. That is, more points are generated where the minority class(es) are sparser, and less points where they are more dense.<br>
Anyway, the class balancing algorithm that ultimately performed better is baseline **SMOTE**,  with some fine tuning on the `sampling_strategy`( the `all` value means that all classes are resampled to match the size of the majority class), and the `n_neighbors` parameters. See [Platform limitations](#limitations)

# **Data Augmentation with Keras `ImageDataGenerator`**

Click the following link for an introduction to [keras Image preprocessing API](https://keras.io/api/preprocessing/image/). Using Keras' ImageDataGenerator, after a manual inspection of the images, we founfd that the best data augmentin technique consisted in a random planar rotation, mixed with random horizontal flip.

# **Model Architecture Exploration**

An extensve esploration of all models has been performed. here we report the *gridsearch* that ultimately proved better.
* Some fine-tuning/exploration of the models' layers and parameters, in particular a dropout layer (EKM only) 
* optimizer variations (EKM)
* optimal dropout and epoch number search
* checkpointing

We couldn't implement Early Stopping both in the EKM model and the DenseNet, as fluctuations in either validation loss, categorical accuracy and column-averaged ROC where too high to set proper `min_delta` and `patience` parameters in [TensorFlow's Early Stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) implementation. With some trial and error, the best optimizer choice for the EKM proved to be the **RMSprop**, while the standard **adam** performed quite well with the DenseNet.
The manual implementations of the dropout and early stopping searches acted simultaneoulsy, so they performed like a grid search. The dropout and epoch values corresponding to the best column-averaged ROC were saved and used during testing phase. Later, in ordere to establish the quantitative impact of weights initialization on the EKM, an EKM model with the best drop is trained and validated, and the best epochs of this model and its analogon trained and validated previously are compared: there was a small difference, so we decided to make three submissions: one with the baseline model re-trained on all data and with th best drop, one with a model formed from the bst weights founfd before, and one with the DenseNet. Besides fluctuatios, we noticed that the DenseNet tends to occasionally reach higher submission scores. 
Considering the fact that optimal epoch number varies with trains et size, a possible third attempt would have seen the best epoch number to use in test phase, when the model is retrained on all train data, extrapolated from a (best-epoch) vs (training set size) plot (given stocasticity was not relevant), but this has unfortunately been impossible due to two reasons:  a techinical difficulty in combining scikit's Leraning curves with a era smodel necessarily trained with generators, and platform RAM limitations.
```{r}
#![Learning rate schedule](/Users/pietromonticone/github/NeuralNetworksProject/Report/Images/LearningRateSchedule.png)
```

### Convolutional AE

Despite the multiple configurations tried, the best we could get is a $0.7$ column-averaged ROC. The reason behind it could be the fact that on one hand an autoencoder with no pooling on the encoder side makes little sense in terms of dimensionality reductions, while on the other hand even a single bidimensonal maxpooling caused the output image to be too little for last EKM layer to classify. See [Platform limitations](#limitations) and see [Model architecture](#model-architecture) . The only way way we managed to at least run it was to build a very shallow autoencoder (just a couple of layers besides the input and the output), with the result  that the loss didn't decrease much. 
Anyway, inspired by the work of others and by some trial and error,  we had a chance to collect some architectural criteria to build an convolutional autoencoder that at least learns.  The folowing  is to be intended as an empirical recipe, with no or little theoretical foundation of the reasons behind its ingredients. The autoencoder is divided in an encoder and a decoder. The encoder should of course start with an input layer, followed by some blocks of Conv2D and Pooling layers (in our case it was MaxPooling2D). Deeper layers should have decreasing filter numbers (for images as big as ours, a range from 64 to 32 should work). The decoder should start with a specular copy of the encoder, where Conv2D layers are substituted by Conv2DTranspose,Pooling by UpSampling. The decoder shall then have as its last two layers  a BatchNormalization layer and  Conv2DTranspose with 3 filters (in order to be able to compare output with input) activated by a sigmoid (this explains the BatchNormalization layer). The unknown numner of Conv2D-pooling blocks in the encoder (that determines the number of Conv2DTranspose-UpSampling in the decoder) has to be jointly concocted with the number of Conv2D-pooling layers of the network (see [Model Architecture](#model-architecture))


```{r autoencoder, echo=FALSE, fig.align='center', fig.cap = "**Figure 2.** Convolutional Autoencoder", message=FALSE, warning=FALSE, paged.print=FALSE}
include_graphics("Images/AEShort.png")
```


### Selected Model Architecture {#model-arcitecture}
Some online resaerch and trial and error with the network architecture gave us some clues about how to build from scratch an effective, dataset dependent model for image classifcation tasks. the network should of course start with a Input layer, followed by blocks of Conv2D-Pooling (MaxPooling in our case) layers. The number of these blocks should be such that the last of them outputs a representation of $n \times n$ pixels ( $\times c \, \,$ channels) where $n$ is of the order of units. This should be then followed by $1-2$ dense layers, and a final dense classifier layer. If The classification is binary (sigmoid), then the last layer should be preceeded by a BatchNormlization layer.

```{r model-architecture, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE}
include_graphics("Images/BaseNetShort.png") 
```

# **Results**

Submissions are evaluated on **mean column-wise ROC AUC**.

* **ROC = 0.956** applying the Keras pre-trained model [DenseNet121](https://keras.io/api/applications/densenet/#densenet121-function);
* **ROC = 0.915** applying a CNN which has been defined and trained from scratch. 

![EKM](Images/ConfusionMatrixEKM.png) ![DenseNet121](Images/ConfusionMatrixDenseNet121.png)




# **Appendix** {.unnumbered}

### Platform Computational Limitations {#limitations}
The newest unstable version of TensorFlow with GPU support is needed to run the code. Unfortunately, we haven't been able to set proper kernels up on our local machines, so we had to rely on publicly available cloud interactive environments like [Kaggle](https://www.kaggle.com/), that provided free out of the box kernels for our purposes. The only limitations are in terms of cpu RAM, which forced us to downsize the images to about $200 \times 200$ pixels.

### Visualization

#### PCA {#pca}

Nonostante si sia poi stabilito di non usare la PCA per la dimensionality reduction del dataset può essere interessante visualizzare i primi dieci layers e confrontarli qualitativamente con una sequenza di layers posti più in profondità. 

![**Figure M.** Layer 1-10 of PCA.](Images/PCA1.png)

![**Figure M+1.** Layer 200-210 of PCA.](Images/PCA200.png)

Come prevedibile risulta più difficile discernere patterns nei layers più profondi (si veda il contrasto tra [Fig. M](#pca1) e [Fig. M+1](#pca200) ).

Di seguito viene visualizzata l'analisi dell'explained variance dove, utilizzando il criterio di conservazione del 90% della varianza, otteniamo 429 componenti.

![](Images/ExplainedVariance90.png)

#### Convolutional Filters and Features Maps {#filters-featuremaps}



![](Images/FiltersEKM.png)

# **References**

1. [Plant Pathology 2020 - FGVC7: Identify the category of foliar diseases in apple trees](https://www.kaggle.com/c/plant-pathology-2020-fgvc7), *Kaggle* (2020). 
1. Ranjita Thapa et al. [The Plant Pathology 2020 challenge dataset to classify foliar disease of apples](https://arxiv.org/abs/2004.11958), *arXiv pre-print* (2020). 
1. Gao Huang et al. [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993), *arXiv pre-print* (2018). 

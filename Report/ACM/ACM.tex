\documentclass{acm_proc_article-sp}
\usepackage[utf8]{inputenc}

\renewcommand{\paragraph}[1]{\vskip 6pt\noindent\textbf{#1 }}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{url}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\title{Dataset Analysis and CNN Models Optimization for Plant Disease
Classification}


% Add imagehandling

\numberofauthors{2}
\author{
\alignauthor Claudio Moroni \\
        \affaddr{University of Turin}\\
       \email{\href{mailto:claudio.moroni@edu.unito.it}{\nolinkurl{claudio.moroni@edu.unito.it}}}
\and \alignauthor Pietro Monticone \\
        \affaddr{University of Turin}\\
       \email{\href{mailto:pietro.monticone@edu.unito.it}{\nolinkurl{pietro.monticone@edu.unito.it}}}
\and \alignauthor Davide Orsenigo \\
        \affaddr{University of Turin}\\
       \email{\href{mailto:davide.orsenigo@edu.unito.it}{\nolinkurl{davide.orsenigo@edu.unito.it}}}
\and }

\date{}

%Remove copyright shit
\permission{}
\conferenceinfo{} {}
\CopyrightYear{}
\crdata{}

% Pandoc syntax highlighting



\begin{document}
\maketitle

\begin{abstract}
We have attended the Kaggle challange \emph{Plant Pathology 2020 -
FGVC7}. In this effort we have trained a convolutional neural network
model with the given training dataset to classify testing images into
different disease categories. During the training phase we have adopted
class balancing, data augmentation, optimal dropout, epoch grid
searching and, wherever possible, we have also manually fine-tuned the
auxiliary elements of the pipeline. The SVD decomposition of the
dataset, the convolutional filters and activation maps have been
visualized. We have ultimately achieved a mean column-wise ROC AUC of
0.937 applying \texttt{EKM}, a relatively shallow CNN defined and
trained from scratch, and 0.972 applying the pre-trained Keras model
\texttt{DenseNet121}, a CNN whose main feature relies on the connection
between layers that are non contiguous (i.e.~the output of the first
layer is not only of the input of the second layer but also the third,
fourth, etc.), which allows for feature reutilization that ultimately
improves performance.\\
\end{abstract}

\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

We have attended the Kaggle challange \emph{Plant Pathology 2020 -
FGVC7}. In this effort we have trained a convolutional neural network
model with the given training dataset to classify testing images into
different disease categories. During the training phase we have adopted
class balancing, data augmentation, optimal dropout, epoch grid
searching and, wherever possible, we have also manually fine-tuned the
auxiliary elements of the pipeline. The SVD decomposition of the
dataset, the convolutional filters and activation maps have been
visualized. We have ultimately achieved a mean column-wise ROC AUC of
0.937 applying \texttt{EKM}, a relatively shallow CNN defined and
trained from scratch, and 0.972 applying the pre-trained Keras model
\texttt{DenseNet121}, a CNN whose main feature relies on the connection
between layers that are non contiguous (i.e.~the output of the first
layer is not only of the input of the second layer but also the third,
fourth, etc.), which allows for feature reutilization that ultimately
improves performance.

\hypertarget{problem}{%
\section{Problem}\label{problem}}

Misdiagnosis of the many diseases impacting agricultural crops can lead
to misuse of chemicals leading to the emergence of resistant pathogen
strains, increased input costs, and more outbreaks with significant
economic loss and environmental impacts. Current disease diagnosis based
on human scouting is time-consuming and expensive, and although
computer-vision based models have the promise to increase efficiency,
the great variance in symptoms due to age of infected tissues, genetic
variations, and light conditions within trees decreases the accuracy of
detection.

\hypertarget{data}{%
\section{Data}\label{data}}

Both the training and the testing datasets are composed of 1821
high-quality, real-life symptom images of multiple apple foliar diseases
to be classified into four categories: \texttt{healthy} (\(h\)),
\texttt{multiple\_diseases} (\(m\)), \texttt{rust} (\(r\)),
\texttt{scab} (\(s\)).

Altough a leaf labeled as \texttt{multiple\_diseases} could be affected
by a variety of diseases including rust, scab or both, we treated the
classes as mutually exclusive because there is no taxonomy: in principle
the model should distinguish between all four classes, as none of them
is an abstraction of any of the others. The dataset is not balanced, but
distributed as follows \((h = 516, m = 91, r = 622, s = 592)\).

\begin{figure}

{\centering \includegraphics[width=800px,height=500]{Images/InputImages} 

}

\caption{**FIGURE 1.** Sample of training images.}\label{fig:input-images}
\end{figure}

Even if we have ultimately decided not to apply the \textbf{PCA} to
reduce the dimensionality of the dataset, we believe it might be
interesting to visualize the first ten principal directions and
qualitatively compare them with a sequence of principal directions with
lower retained variance. As we can appreciate in the figures reported in
the \protect\hyperlink{pca}{Appendix}, the principal components with
lower retained variance correspond to almost pure noise and from the
retained variance assesment (using the criterion of 90\% variance
retention) we have obtained 429 components.

Here instead we visualize the first two principal components of a
\textbf{truncated SVD} to qualitatively investigate the linear
separability of the dataset \footnote{\textbf{Assumption}: if the
  dataset is linearly separable, the direction along which which the
  classes diverge is one of the principal components with larger
  retained variance, otherwise the noise would be greater than the
  signal.}.

\begin{figure}

{\centering \includegraphics[width=450px,height=450]{Images/TruncatedSVD_preSMOTE} 

}

\caption{**FIGURE 2.** Pre-`SMOTE` truncated SVD.}\label{fig:pre-smote}
\end{figure}

As one could have resonably expected given such a high dimensionality,
the dataset is not linearly separable.

Later in the report we will describe an attempt using a
\protect\hyperlink{ae}{convolutional autoencoder}, while in the next
section we can verify the amplification of the classes performed by
\texttt{SMOTE} and recognize the clustering of the generated points.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{smote}{%
\subsection{\texorpdfstring{Class Balancing with
\texttt{SMOTE}}{Class Balancing with SMOTE}}\label{smote}}

\texttt{SMOTE(sampling\_strategy,\ k\_neighbors)} is a class balancing
algorithm that operates as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (one of) the minority class(es) is considered ;
\item
  a point is randomly chosen and its first \texttt{n\_neighbors} nearest
  neighbors are found ;
\item
  one of those nearest neighbors is then randomly selected, and the
  vector between this point and the originally selected point is drawn ;
\item
  this vector is multiplied by a number between 0 and 1, and the
  resulting synthetic point is added to the dataset.
\end{enumerate}

Besides the baseline variant, \texttt{SVMSMOTE} and \texttt{ADASYN} have
been tested too:

\begin{itemize}
\tightlist
\item
  \texttt{SVMSMOTE} starts by fitting an SVM on the data, identifies the
  points which are more prone to mis-classification (i.e.~those on the
  border of the class cluster) via its support vectors and then will
  oversample those points more than the others.
\item
  \texttt{ADASYN} instead draws from a distribution over the minority
  class(es) that is pointwise inversely proportional to their density,
  so that more points are generated where the minority class(es) are
  sparser, and less points where they are more dense.
\end{itemize}

We have obtained the best performance applying baseline \texttt{SMOTE}
with some fine-tuning on the \texttt{sampling\_strategy} \footnote{The
  value \texttt{all} means that all classes are resampled to match the
  size of the majority class.} and the \texttt{n\_neighbors} parameters.
For more details see \protect\hyperlink{platform-limitations}{Platform
Limitations}.

\begin{figure}

{\centering \includegraphics[width=450px,height=450]{Images/TruncatedSVD_postSMOTE} 

}

\caption{**FIGURE 3.** Post-`SMOTE` truncated SVD.}\label{fig:post-smote}
\end{figure}

\hypertarget{data-generator}{%
\subsection{\texorpdfstring{Data Augmentation with Keras
\texttt{ImageDataGenerator}}{Data Augmentation with Keras ImageDataGenerator}}\label{data-generator}}

We have adopted the Keras \texttt{ImageDataGenerator} and, after a
manual inspection of the images, we found that the best data
augmentation technique was a random planar rotation combined with random
horizontal flip. \footnote{For further information read the
  \href{https://keras.io/api/preprocessing/image/}{Keras image
  pre-processing API}.}

\hypertarget{exploration}{%
\subsubsection{Model Architecture Exploration}\label{exploration}}

We have implemented an extensive exploration of all models and here is
reported the \textbf{grid search} that achieved the best performance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  some exploration and fine-tuning of the layers and parameters of the
  models (i.p. a dropout layer for the \texttt{EKM} only) ;
\item
  variations of the optimizer (for the \texttt{EKM} only) ;
\item
  optimal dropout and epoch number search ;
\item
  checkpointing .
\end{enumerate}

We couldn't implement \textbf{early stopping} both in \texttt{EKM} and
\texttt{DenseNet121}, since the fluctuations in either validation loss,
categorical accuracy or mean column-wise ROC AUC where too high to
properly set the \texttt{min\_delta} and \texttt{patience} parameters in
the
\href{https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping}{TensorFlow
implementation}.

The best choice of the optimizer for the \texttt{EKM} proved to be the
\texttt{RMSprop}, while the standard \texttt{adam} performed pretty well
with the \texttt{DenseNet121}. The manual implementations of the dropout
and early stopping searches acted simultaneoulsy, so they performed like
a grid search. The dropout, epoch values and weight corresponding to the
highest mean column-wise ROC AUC were saved and used during the testing
phase.

Then, in order to establish the quantitative impact of stocasticity in
the initialization of the weights on \texttt{EKM}, an \texttt{EKM1} with
the best drop is trained and validated, and the best epochs of
\texttt{EKM} and \texttt{EKM1} are compared. There was a small
difference, therefore we decided to make three submissions: one with the
baseline \texttt{EKM} re-trained on all the data and with the best drop,
one with \texttt{EKM1} and one with the \texttt{DenseNet121}.

Besides fluctuations, we have noticed that the \texttt{DenseNet121}
tends to reach higher submission scores. See
\protect\hyperlink{training-histories}{Training Histories}.

\hypertarget{ae}{%
\subsection{Convolutional Autoencoder}\label{ae}}

The best we could achieve by inserting a covolutional autoencoder
between the smoted data, augmented data and the model training is a
\(0.7\) mean column-wise ROC AUC, despite the large number of the
configurations that have been tried. The reason behind this relatively
poor performance could be that on the one hand an autoencoder with no
pooling on the encoder side makes little sense in terms of
dimensionality reduction, while on the other hand even a single
bidimensonal maxpooling caused the output image to be too little for the
last \texttt{EKM} layer to classify. See
\protect\hyperlink{limitations}{Platform limitations} and
\protect\hyperlink{model-architecture}{Model Architecture Exploration}.

The only way way we have managed to run it and see at least some loss
drop was to build a very shallow autoencoder (i.e.~just a couple of
layers besides the input and the output), with the result that the loss
didn't decrease significantly. Anyway, inspired by the work of others
and by some active trial and error, we have had a chance to collect some
architectural criteria to build a convolutional autoencoder that at
least exhibits learning. The following is to be intended as an empirical
recipe, with no or little theoretical foundation supporting the choice
of its ingredients.

The autoencoder is composed of an encoder and a decoder. Obviously the
encoder should start with an \texttt{input} layer, followed by some
blocks of \texttt{Conv2D} and \texttt{Pooling} layers. Deeper layers
should have decreasing filter numbers (for images as big as ours, a
range from 64 to 32 should work). The decoder should start with a
specular copy of the encoder, where \texttt{Conv2D} layers are
substituted by \texttt{Conv2DTranspose} and \texttt{Pooling} by
\texttt{UpSampling}. Then the last two layers of the decoder should be a
\texttt{BatchNormalization} layer and \texttt{Conv2DTranspose} with 3
filters (in order to be able to compare output with input) activated by
a sigmoid (which explains the \texttt{BatchNormalization\ layer}). The
unknown number of \texttt{Conv2D}-\texttt{Pooling} blocks in the encoder
(that determines the number of
\texttt{Conv2DTranspose}-\texttt{UpSampling} in the decoder) has to be
jointly connected with the number of \texttt{Conv2D}-\texttt{Pooling}
layers of the network. See \protect\hyperlink{model-architecture}{Model
Architecture}.

\begin{figure}

{\centering \includegraphics[width=350px,height=500]{Images/AEShort} 

}

\caption{**FIGURE 4.** Schematic representation of the convolutional autoencoder.}\label{fig:ae}
\end{figure}

\hypertarget{model-architecture}{%
\subsection{Selected Model Architecture}\label{model-architecture}}

Some online resaerch and active trial and error with the network
architecture gave us some clues about how to build from scratch an
effective, dataset-dependent model for image classification.

Obviously the network should start with a \texttt{input} layer, followed
by blocks of \texttt{Conv2D}-\texttt{Pooling}\footnote{\texttt{MaxPooling}
  in our case.} layers. The number of these blocks should be such that
the last of them outputs a representation of \(n \times n\) pixels (
\(\times c\) channels) where \(n\) is of the order of units. Then this
should be followed by 1-2 dense layers, and a final dense classifier
layer. If the classification is binary (sigmoid), then the last layer
should be preceeded by a \texttt{BatchNormalization} layer.

\begin{figure}

{\centering \includegraphics[width=350px,height=500]{Images/BaseNetShort} 

}

\caption{**FIGURE 5.** Schematic representation of the selected model architecture.}\label{fig:basenet}
\end{figure}

\hypertarget{results}{%
\section{Results}\label{results}}

The performance of the models has been evaluated on \textbf{mean
column-wise ROC AUC}: 0.972 for \texttt{DenseNet121} and 0.937 for
\texttt{EKM}.

\begin{figure}

{\centering \includegraphics[width=800px,height=550]{ACM_files/figure-latex/confusion-matrices-1} 

}

\caption{**FIGURE 6.** Confusion matrices of `EKM` (left) and `DenseNet121` (right).}\label{fig:confusion-matrices}
\end{figure}

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

Since the optimal epoch number varies with the size of training dataset,
a possible third attempt to obtain it would have seen the best epoch
number to use in the testing phase, when the model is re-trained on all
training data, extrapolated from a
\texttt{(best-epoch,\ training-set-size)} plot (given that stocasticity
has not been relevant).

This has been practically impossible for us because of two main reasons:
technical difficulty in combining Sci-kit learning curves with a Keras
model necessarily trained with generators, and
\protect\hyperlink{limitations}{platform limitations}. Those limitations
prevented us from instantiating a single \texttt{Pipeline} object
integrating all the elements (\texttt{SMOTE},
\texttt{ImageDataGenerator}, \texttt{Model}): this could have allowed us
to perform a more extensive and reliable\footnote{If coupled with
  \emph{cross validation} instead of 80\%-20\% splitting.} grid search.
Finally, as we have already mentioned, those computational limitations
prevented us from implementing an effective convolutional autoencoder:
if we used the full-sized images, the autoencoder may have been deeper
and that could have plausibly yielded a better performance.

\hypertarget{references}{%
\section{References}\label{references}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://www.kaggle.com/c/plant-pathology-2020-fgvc7}{Plant
  Pathology 2020 - FGVC7: Identify the category of foliar diseases in
  apple trees}, \emph{Kaggle} (2020).
\item
  Ranjita Thapa et al. \href{https://arxiv.org/abs/2004.11958}{The Plant
  Pathology 2020 challenge dataset to classify foliar disease of
  apples}, \emph{arXiv pre-print} (2020).
\item
  Gao Huang et al. \href{https://arxiv.org/abs/1608.06993}{Densely
  Connected Convolutional Networks}, \emph{arXiv pre-print} (2018).
\end{enumerate}

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{additional-material}{%
\subsection{Related Material}\label{additional-material}}

\begin{itemize}
\tightlist
\item
  Explore the
  \href{https://github.com/InPhyT/NeuralNetworksProject}{GitHub
  repository} of the project.
\item
  Read the code in the
  \href{https://nbviewer.jupyter.org/github/InPhyT/NeuralNetworksProject/notebook.ipynb}{Jupyter
  notebook}.
\item
  Run the code in the
  \href{https://www.kaggle.com/inphyt2020/neuralnetworksproject}{Kaggle
  notebook}.
\end{itemize}

\hypertarget{platform-limitations}{%
\subsection{Platform Limitations}\label{platform-limitations}}

Since the last unstable version of GPU-supported TensorFlow is required
to run the code and we haven't been able to set the proper kernels up on
our local machines, we have been constrained to rely on a publicly
available cloud interactive environment like \emph{Kaggle}, which
provided free out of the box kernels for our purposes. The only
limitations are in terms of CPU RAM, which forced us to downsize the
images to about 200 \(\times\) 200 pixels.

\hypertarget{visualization}{%
\subsection{Visualization}\label{visualization}}

\hypertarget{pca}{%
\subsubsection{PCA}\label{pca}}

\begin{figure}

{\centering \includegraphics[width=800px,height=400]{Images/PCA1} 

}

\caption{**FIGURE 7.** The first ten principal directions of the PCA.}\label{fig:pca1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=800px,height=400]{Images/PCA200} 

}

\caption{**FIGURE 8.** Directions 200-210 of the PCA.}\label{fig:pca200}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=550px,height=400]{Images/ExplainedVariance90} 

}

\caption{**FIGURE 9.** Explained Variance.}\label{fig:variance}
\end{figure}

\hypertarget{training-histories}{%
\subsubsection{Training Histories}\label{training-histories}}

\begin{figure}

{\centering \includegraphics[width=20.25in]{Images/TrainingHistoryEKM} 

}

\caption{**FIGURE 10.** Training history of the `EKM`.}\label{fig:ekm-training}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=19.81in]{Images/TrainingHistoryComparison} 

}

\caption{**FIGURE 11.** Training history of the `EKM1`.}\label{fig:ekm1-training}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=19.56in]{Images/TrainingHistoryDenseNet121} 

}

\caption{**FIGURE 12.** Training history of the `DenseNet121`.}\label{fig:densenet-training}
\end{figure}

\hypertarget{filters-featuremaps}{%
\subsubsection{Filters and Activation Maps}\label{filters-featuremaps}}

\begin{figure}

{\centering \includegraphics[width=550px,height=460]{Images/FiltersEKM} 

}

\caption{**FIGURE 13.** Three channels with some of the first 3x3 filters of the first hidden layer of the `EKM`.}\label{fig:filters-ekm}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=9.5in]{Images/FeatureMaps2} 

}

\caption{**FIGURE 14.** Activation maps of the second hidden layer of `EKM`.}\label{fig:maps2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=9.5in]{Images/FeatureMaps6} 

}

\caption{**FIGURE 15.** Activation maps of the sixth hidden layer of `EKM`.}\label{fig:maps6}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=9.5in]{Images/FeatureMaps9} 

}

\caption{**FIGURE 13.** Activation maps of the ninth hidden layer of `EKM`.}\label{fig:maps9}
\end{figure}
\setlength{\parindent}{0in}

\end{document}

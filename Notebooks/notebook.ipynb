{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "\n",
    "- [Challenge](https://www.kaggle.com/c/plant-pathology-2020-fgvc7/data?select=images)\n",
    "- [fit_generator (1)](https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/)\n",
    "- [GPU (1)](https://forums.fast.ai/t/problem-with-fit-generator-in-keras-2-using-tensorflow/2877/5)\n",
    "- [Kaggle example (1)](https://www.kaggle.com/aniruddhakalkar/plant-village-disease-classification)\n",
    "- [Kaggle example (2)](https://www.kaggle.com/tathagatbanerjee/final-plant-village-transfer-learning-analysis)\n",
    "- [Kaggle example (3)](https://www.kaggle.com/layediop/plant-pathology)\n",
    "\n",
    "- [Kaggle example (4)](https://www.kaggle.com/nightwolfbrooks/data-augmentation-and-keras-cnn)\n",
    "\n",
    "- [Kaggle example (5)](https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models)\n",
    "\n",
    "- [kaggle example (6)](https://www.kaggle.com/prateek0x/multiclass-image-classification-using-keras)\n",
    "\n",
    "- [manage jpg files](https://stackoverflow.com/questions/11903037/copy-all-jpg-file-in-a-directory-to-another-directory-in-python)\n",
    "\n",
    "- [keras variable size input (1)](https://stackoverflow.com/questions/47795697/how-to-give-variable-size-images-as-input-in-keras)\n",
    "\n",
    "- [keras variable size input (2)](https://github.com/keras-team/keras/issues/1920)\n",
    "\n",
    "- [gfg basic keras image classification tutorial](https://www.geeksforgeeks.org/python-image-classification-using-keras/#:~:text=Image%20classification%20is%20a%20method,of%20the%20model%20using%20VGG16)\n",
    "\n",
    "- [keras for R](https://www.shirin-glander.de/2018/06/keras_fruits/)\n",
    "\n",
    "- [tensorflow basic keras image classification](https://www.tensorflow.org/tutorials/keras/classification?hl=it)\n",
    "\n",
    "- [keras basic image classification tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) \n",
    "\n",
    "- [github example](https://github.com/vijayg15/Keras-MultiClass-Image-Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "## Train & Validation, Test Splitting\n",
    "\n",
    "This section has been run previously. The output dataset can be found in the input folder\n",
    "\n",
    "```python\n",
    "src_dir = \"../input/plantvillage/images\" #r\"../plant-pathology-2020-fgvc7/images\"\n",
    "train_val_dst_dir = \"../working/train&val_images\" #r\"../plant-pathology-2020-fgvc7/train&val_images\" #\"../working/train&val_images\"\n",
    "test_dst_dir = \"../working/test_images\" #r\"../plant-pathology-2020-fgvc7/test_images\"  #\"../working/test_images\"\n",
    "if not os.path.isdir(train_val_dst_dir):\n",
    "    os.mkdir(train_val_dst_dir)\n",
    "if not os.path.isdir(test_dst_dir):\n",
    "    os.mkdir(test_dst_dir)\n",
    "\n",
    "\n",
    "if len([f for f in os.listdir(test_dst_dir)]) == 0:\n",
    "\n",
    "\n",
    "    all_images_names = os.listdir(src_dir)\n",
    "    train_val_images = []\n",
    "    test_images  = []\n",
    "    for image in all_images_names:\n",
    "        if \"Train\" in image:\n",
    "            shutil.copy(src_dir+\"/\"+image,train_val_dst_dir)\n",
    "        elif \"Test\" in image:\n",
    "            shutil.copy(src_dir+\"/\"+image,test_dst_dir)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "# check for possible errors\n",
    "total = len([f for f in  os.listdir(src_dir)])\n",
    "train_val_total = len([f for f in  os.listdir(train_val_dst_dir)])\n",
    "test_total = len([f for f in  os.listdir(test_dst_dir)])\n",
    "print(total == train_val_total + test_total)\n",
    "```\n",
    "[] : True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install latest version f tensorflow, which comes with useful image loading APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only once per session\n",
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE # to balance classes\n",
    "\n",
    "\n",
    "# plot model histories\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Here we get an insight of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = True\n",
    "if kaggle:\n",
    "    outdir = r\"../working/\"\n",
    "    indir = r\"../input/plantvillage/\"\n",
    "else:\n",
    "    outdir = r\"Q:/tooBigToDrive/plantsvillage/temp/\"\n",
    "    indir = r\"Q:/tooBigToDrive/plantsvillage/\"\n",
    "    \n",
    "test_dir = indir + \"plantvillage_split_dataset/test_images\"\n",
    "train_labels_csv = pd.read_csv(indir+\"plantvillage_split_dataset/train.csv\")\n",
    "print(train_labels_csv.head())\n",
    "print(\"-------------------------------------\")\n",
    "example_submission_csv = pd.read_csv(indir+\"plantvillage_split_dataset/sample_submission.csv\")\n",
    "print(example_submission_csv.head() )\n",
    "print(\"-------------------------------------\")\n",
    "test_csv = pd.read_csv(indir+\"plantvillage_split_dataset/test.csv\")\n",
    "test_paths_csv= pd.DataFrame(test_csv[\"image_id\"].apply(lambda x: test_dir+\"/\"+x+\".jpg\"))\n",
    "print(test_paths_csv.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation Splitting\n",
    "This cell creates 4 directories (healthy, multiple_diseases, rust,scab) with the corresponding images from the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csvs with images\n",
    "train_val_healthy_csv = train_labels_csv[train_labels_csv[\"healthy\"] == 1]\n",
    "train_val_multiple_diseases_csv  = train_labels_csv[train_labels_csv[\"multiple_diseases\"] == 1]\n",
    "train_val_rust_csv = train_labels_csv[train_labels_csv[\"rust\"] == 1]\n",
    "train_val_scab_csv = train_labels_csv[train_labels_csv[\"scab\"] == 1]\n",
    "# we will check that this has no entry\n",
    "two_classes = train_labels_csv[(train_labels_csv[\"scab\"] == 1) & (train_labels_csv[\"multiple_diseases\"] == 1)]\n",
    "\n",
    "train_val_healthy_names = train_val_healthy_csv[\"image_id\"].tolist()\n",
    "train_val_multiple_diseases_names = train_val_multiple_diseases_csv[\"image_id\"].tolist()\n",
    "train_val_rust_names = train_val_rust_csv[\"image_id\"].tolist()\n",
    "train_val_scab_names = train_val_scab_csv[\"image_id\"].tolist()\n",
    "\n",
    "src_dir = indir+\"plantvillage_split_dataset/train&val_images\" #\"../input/plantvillage/images\"\n",
    "train_dst_dir = outdir+\"train\" #\"../working/train&val_images\"\n",
    "\n",
    "#val_dst_dir = outdir+\"val\"  #\"../working/test_images\"\n",
    "train_dst_healthy_dir = outdir+\"train/healthy\"#\"../working/train&val_images\"\n",
    "train_dst_multiple_diseases_dir =outdir+\"train/multiple_diseases\"\n",
    "train_dst_rust_dir = outdir+\"train/rust\"\n",
    "train_dst_scab_dir = outdir+\"train/scab\"\n",
    "\n",
    "test_dst_dir = outdir + \"test_image/test\"\n",
    "\n",
    "# crreate teh directories and fill them\n",
    "try:\n",
    "    os.mkdir(train_dst_dir)\n",
    "\n",
    "    os.mkdir(train_dst_healthy_dir)\n",
    "    os.mkdir(train_dst_multiple_diseases_dir)\n",
    "    os.mkdir(train_dst_rust_dir)\n",
    "    os.mkdir(train_dst_scab_dir)\n",
    "    os.makedirs(test_dst_dir)\n",
    "\n",
    "    for image in train_val_healthy_names :\n",
    "            shutil.copy(src_dir+\"/\"+image+\".jpg\",train_dst_healthy_dir)\n",
    "            \n",
    "    for image in train_val_multiple_diseases_names :\n",
    "            shutil.copy(src_dir+\"/\"+image+\".jpg\",train_dst_multiple_diseases_dir)\n",
    "\n",
    "    for image in train_val_rust_names :\n",
    "            shutil.copy(src_dir+\"/\"+image+\".jpg\",train_dst_rust_dir)\n",
    "\n",
    "    for image in train_val_scab_names :\n",
    "            shutil.copy(src_dir+\"/\"+image+\".jpg\",train_dst_scab_dir)\n",
    "\n",
    "    for image in test_paths_csv[\"image_id\"].tolist():\n",
    "        shutil.copy(image,test_dst_dir)\n",
    "        \n",
    "except FileExistsError as err:\n",
    "    print(\"folders already exist\")\n",
    " \n",
    "#check for possible errors\n",
    "total = len([f for f in  os.listdir(src_dir)])\n",
    "train_healthy_total = len([f for f in  os.listdir(train_dst_healthy_dir)])\n",
    "train_multiple_diseases_total = len([f for f in  os.listdir(train_dst_multiple_diseases_dir)])\n",
    "train_rust_total = len([f for f in  os.listdir(train_dst_rust_dir)])\n",
    "train_scab_total = len([f for f in  os.listdir(train_dst_scab_dir)])\n",
    "\n",
    "total = train_healthy_total + train_multiple_diseases_total +train_rust_total+ train_scab_total #steps_per_epoch = \n",
    "train_size = math.ceil(total*0.8)\n",
    "val_size = total - train_size\n",
    "test_size = test_csv.size \n",
    "image_size  = (200,200)\n",
    "batch_size = 32\n",
    "print(train_healthy_total,train_multiple_diseases_total,train_rust_total,train_scab_total,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class balancing and Data Augmentation/Prepocessing\n",
    "This function performs class balancing and data augmentation. Classes are balanced with SMOTE, while data augmentation is performed with tendorflow's ImageDataGenerator. <br>\n",
    "Multiple variants of SMOTE and ImageDataGenerator have been tried, resulting in the following optimal configuration which also cares about performance. <br>\n",
    "Note that the function is split in two by the `valid` parameter: it allows to use the function to produce both the train and validation data and the train only. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_data(train_dst_dir , train_generator, val_generator, aug, batch_size, valid = True):\n",
    "    if valid:\n",
    "        # load data into tensorflow dataset: if we used the flow_form_directory method of the train_generator, it would go too slow\n",
    "        print(\"loading data...\")\n",
    "        train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        train_dst_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=train_size, #batch_size\n",
    "        )\n",
    "\n",
    "        val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            train_dst_dir,\n",
    "            #val_dst_dir,\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\",\n",
    "            seed=1337,\n",
    "            image_size=image_size,\n",
    "            batch_size=val_size,\n",
    "        )\n",
    "        \n",
    "        print(\"augmenting train...\")\n",
    "        res = list(zip(*train_ds.unbatch().as_numpy_iterator()))\n",
    "        x_train = np.array(res[0])\n",
    "        print(\"x done\")\n",
    "        y_train = np.array(res[1])\n",
    "        print(x_train.shape,y_train.shape)\n",
    "    \n",
    "        x_train  = np.array([image.flatten() for image in x_train ])\n",
    "        print(\"flattened\")\n",
    "\n",
    "        smote_train = SMOTE(sampling_strategy = \"all\", random_state = 420,k_neighbors=10,n_jobs=4)   #svmsmote goues aout of memory in all configs\n",
    "        x_train, y_train = smote_train.fit_resample(x_train, y_train)\n",
    "        x_train = np.reshape(x_train,(-1,200,200,3))\n",
    "        tot_train = len(x_train)\n",
    "        print(\"total_train after adasyn = \", x_train.shape)\n",
    "        \n",
    "        y_train_cat = tf.keras.utils.to_categorical(\n",
    "            y_train, num_classes=4, dtype='float32'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        train_generator.fit(x_train)\n",
    "        aug_train_images, aug_train_labels = train_generator.flow(x = x_train,y = y_train_cat,shuffle = False,batch_size = tot_train).next() #batch_size = train_size\n",
    "        aug_train_images = np.array(aug_train_images)\n",
    "        aug_train_labels = np.array(aug_train_labels)\n",
    "        \n",
    "        # save memory\n",
    "        del x_train\n",
    "        #del y_train\n",
    "        del train_ds\n",
    "\n",
    "        out_train_datagen = ImageDataGenerator()\n",
    "        out_train_datagen.fit(aug_train_images)\n",
    "        out_train_flow = out_train_datagen.flow(aug_train_images,aug_train_labels,batch_size = batch_size,shuffle = False)\n",
    "\n",
    "        del aug_train_images\n",
    "        del aug_train_labels\n",
    "\n",
    "        print(\"train augmented, augmenting val...\")\n",
    "        #i = 0\n",
    "        res = list(zip(*val_ds.unbatch().as_numpy_iterator()))\n",
    "        x_val = np.array(res[0])\n",
    "        y_val = np.array(res[1])\n",
    "        y_val_cat = tf.keras.utils.to_categorical(\n",
    "            y_val, num_classes=4, dtype='float32'\n",
    "        )\n",
    "        print(x_val.shape,y_val.shape,y_val_cat.shape)\n",
    "        \n",
    "        \n",
    "        val_generator.fit(x_val)\n",
    "        aug_val_images, aug_val_labels = val_generator.flow(x = x_val,y = y_val_cat,shuffle = False,batch_size = val_size).next()\n",
    "        aug_val_images = np.array(aug_val_images)\n",
    "        aug_val_labels = np.array(aug_val_labels)\n",
    "\n",
    "        del x_val\n",
    "        del val_ds\n",
    "\n",
    "        out_val_datagen = ImageDataGenerator()\n",
    "        out_val_datagen.fit(aug_val_images)\n",
    "        out_val_flow = out_val_datagen.flow(aug_val_images,aug_val_labels,batch_size = val_size, shuffle = False)\n",
    "\n",
    "        del aug_val_images\n",
    "        del aug_val_labels\n",
    "        del res\n",
    "\n",
    "        print(\"returning\")\n",
    "        return (out_train_flow,out_val_flow,y_val,y_train,tot_train)\n",
    "    # if validation is not provided/ one intends to test\n",
    "    else:\n",
    "        print(\"loading data...\")\n",
    "        train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        train_dst_dir,\n",
    "        #validation_split=0.2,\n",
    "        #subset=\"training\",\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=train_size, #batch_size\n",
    "        )\n",
    "        \n",
    "        print(\"augmenting train...\")\n",
    "        res = list(zip(*train_ds.unbatch().as_numpy_iterator()))\n",
    "        x_train = np.array(res[0])\n",
    "        y_train = np.array(res[1])\n",
    "        print(x_train.shape,y_train.shape)\n",
    "        \n",
    "        x_train  = np.array([image.flatten() for image in x_train ])\n",
    "        print(\"flattened\")\n",
    "\n",
    "        smote_train = SMOTE(sampling_strategy = \"all\", random_state = 420,k_neighbors=10,n_jobs=4)\n",
    "        x_train, y_train = smote_train.fit_resample(x_train, y_train)\n",
    "        x_train = np.reshape(x_train,(-1,200,200,3))\n",
    "        \n",
    "        tot_train = len(x_train)\n",
    "        print(\"total_train after adasyn = \", x_train.shape)\n",
    "        \n",
    "        y_train_cat = tf.keras.utils.to_categorical(\n",
    "            y_train, num_classes=4, dtype='float32'\n",
    "        )   \n",
    "\n",
    "        train_generator.fit(x_train)\n",
    "        aug_train_images, aug_train_labels = train_generator.flow(x = x_train,y = y_train_cat,shuffle = False,batch_size = tot_train).next() #batch_size = train_size\n",
    "        aug_train_images = np.array(aug_train_images)\n",
    "        aug_train_labels = np.array(aug_train_labels)\n",
    "\n",
    "        del x_train\n",
    "        del y_train\n",
    "        del train_ds\n",
    "\n",
    "        out_train_datagen = ImageDataGenerator()\n",
    "        out_train_datagen.fit(aug_train_images)\n",
    "        out_train_flow = out_train_datagen.flow(aug_train_images,aug_train_labels,batch_size = batch_size,shuffle = False)\n",
    "\n",
    "        del aug_train_images\n",
    "        del aug_train_labels\n",
    "        \n",
    "        return (out_train_flow,tot_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set preprocessing\n",
    "The test set is preprocesses just as the validation set, in order to give the model the same feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_augmented_test(test_dir, test_generator):\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        outdir + \"test_image\",\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=None,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=test_size,\n",
    "        image_size=image_size,\n",
    "        shuffle = False,\n",
    "        seed=None,\n",
    "        validation_split=None,\n",
    "        subset=None,\n",
    "        interpolation=\"bilinear\",\n",
    "        follow_links=False,\n",
    "    )\n",
    "    \n",
    "    x_test = np.array([ array for array, label in test_ds.unbatch().as_numpy_iterator()])\n",
    "    test_generator.fit(x_test)\n",
    "    test_flow = test_generator.flow(\n",
    "        x= x_test,\n",
    "        y=None,\n",
    "        batch_size = test_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    test_imgs = test_flow.next()\n",
    "\n",
    "    del test_ds\n",
    "    del x_test\n",
    "    del test_generator\n",
    "\n",
    "    return test_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators definition\n",
    "\n",
    "Visit https://keras.io/api/preprocessing/image/#imagedatagenerator-class for further details. One must note that preprocessing is different from augmenting: the former is referred to an information-preserving transformtion applied to all data, while the latter to a random modification (so  information could be lost) applied to a random sample of the data. So, (selected) augmenting techniques are applied to train only, while validation and test set receive just the preprocessing applied to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "train_datagen = ImageDataGenerator(rotation_range=360,                # DATA AUGMENTATION\n",
    "                                   #shear_range=.25,                  # DATA AUGMENTATION\n",
    "                                   #zoom_range=.25,                   # DATA AUGMENTATION\n",
    "                                   #width_shift_range=.25,            # DATA AUGMENTATION\n",
    "                                   #height_shift_range=.25,           # DATA AUGMENTATION\n",
    "                                   rescale=1/255,                    # DATA MODIFICATION\n",
    "                                   #brightness_range=[.5,1.5],        # DATA AUGMENTATION\n",
    "                                   horizontal_flip=True,             # DATA AUGMENTATION\n",
    "                                   vertical_flip=True                # DATA AUGMENTATION\n",
    "                                  )\n",
    "\n",
    "# VALIDATION\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# TEST\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# To train and validate\n",
    "train_flow, val_flow, y_val, y_train,total_train = create_augmented_data(train_dst_dir  = train_dst_dir,train_generator = train_datagen, val_generator = val_datagen , aug = 5, batch_size = batch_size )\n",
    "#test_imgs = get_augmented_test(test_dir = test_dir, test_generator = test_datagen)\n",
    "test_imgs = get_augmented_test(test_dir = test_dir, test_generator = test_datagen)\n",
    "print(test_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un'altra rete\n",
    "def get_model():\n",
    "    \n",
    "    METRICS = [ \n",
    "      tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy'),\n",
    "      tf.keras.metrics.AUC(name='categorical_auc',multi_label=True),\n",
    "        ]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "       # tf.keras.Input(shape=(150, 150, 3)),\n",
    "        #data_augmentation(inputs),\n",
    "    #     tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    #     tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    #     tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        # Note the input shape is the desired size of the image 200x 200 with 3 bytes color\n",
    "        # The first convolution\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(200, 200, 3)), #, input_shape=(150, 150, 3)\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The second convolution\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # The third convolution\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # The fourth convolution\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # The fifth convolution\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        # Flatten the results to feed into a dense layer\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # 128 neuron in the fully-connected layer\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        # 5 output neurons for 5 classes with the softmax activation\n",
    "        tf.keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    #model.compile(loss='categorical_crossentropy', metrics = ['categorical_accuracy'],optimizer='adam')\n",
    "    model.compile(loss='categorical_crossentropy', metrics = METRICS ,optimizer='adam')\n",
    "\n",
    "    # Model Summary\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "history = model.fit_generator(train_flow,\n",
    "            steps_per_epoch = total_train // batch_size, #train_size//batch_size\n",
    "            epochs=15, # the model never seems to suffer from validation loss increase (even up to 100 epochs)\n",
    "            validation_data=val_flow,\n",
    "            validation_steps=1,\n",
    "            #callbacks=callbacks,        # we tried early stopping and learning rate scheduling, but they proved inefficient due to the high loss swipes we had during training.\n",
    "            workers=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "        \n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(1, 100+1), mode='lines+markers', y=history.history['categorical_accuracy'], marker=dict(color=\"dodgerblue\"),\n",
    "            name=\"Train acc\"))\n",
    "    \n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(1, 100+1), mode='lines+markers', y=history.history['val_categorical_accuracy'], marker=dict(color=\"darkblue\"),\n",
    "            name=\"Val acc\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(1, 100+1), mode='lines+markers', y=history.history['categorical_auc'], marker=dict(color=\"orange\"),\n",
    "            name=\"Train auc\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(1, 100+1), mode='lines+markers', y=history.history['val_categorical_auc'], marker=dict(color=\"orangered\"),\n",
    "            name=\"Val auc\"))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(title_text=\"\", yaxis_title= \"Metrics\", xaxis_title=\"Epochs\", template=\"plotly_white\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_generator(val_flow, train_size // batch_size +1) #128 +1\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "a = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plot_confusion_matrix(a,[\"h\",\"d\",\"c\",\"f\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on all data and predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict test\n",
    "\n",
    "model1 = get_model()\n",
    "\n",
    "train_flow,total_train = create_augmented_data(train_dst_dir  = train_dst_dir,train_generator = train_datagen, val_generator = val_datagen , aug = 5, batch_size = 32, valid = False )\n",
    "model1.fit_generator(train_flow,\n",
    "            steps_per_epoch = total_train // batch_size, #train_size//batch_size\n",
    "            epochs=20,\n",
    "            #callbacks=callbacks,\n",
    "            workers=4)\n",
    "\n",
    "def tensorSort(data):\n",
    "    return sorted(data, key=lambda item: (int(item.partition(' ')[0])\n",
    "                               if item[0].isdigit() else float('inf'), item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_predicted = model.predict_generator(test_flow, 1)\n",
    "y_predicted = model.predict(test_imgs)\n",
    "submission = pd.DataFrame(y_predicted, columns = [\"healthy\", \"multiple_diseases\", \"rust\",\"scab\"],)\n",
    "submission.insert(0,\"image_id\",tensorSort(test_csv[\"image_id\"].tolist()))\n",
    "submission.to_csv(\"../working/submission.csv\", index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters and Feature Maps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import glob\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image as pil_image\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "# Import module we'll need to import our custom module\n",
    "from shutil import copyfile\n",
    "\n",
    "# Copy our file into the working directory (make sure it has .py suffix)\n",
    "copyfile(src = \"../input/filter-visualization-modules/conv_filter_visualization.py\", dst = \"../working/conv_filter_visualization.py\")\n",
    "copyfile(src = \"../input/othermodules/model.py\", dst = \"../working/model.py\")\n",
    "copyfile(src = \"../input/othermodules/viz.py\", dst = \"../working/viz.py\")\n",
    "copyfile(src = \"../input/othermodules/utils.py\", dst = \"../working/utils.py\")\n",
    "\n",
    "# Import all functions\n",
    "from conv_filter_visualization import *\n",
    "\n",
    "# Models\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Visualization (VGG16 Keras Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import glob\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image as pil_image\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "# Import module we'll need to import our custom module\n",
    "from shutil import copyfile\n",
    "\n",
    "# Copy our file into the working directory (make sure it has .py suffix)\n",
    "copyfile(src = \"../input/filter-visualization-modules/conv_filter_visualization.py\", dst = \"../working/conv_filter_visualization.py\")\n",
    "copyfile(src = \"../input/othermodules/model.py\", dst = \"../working/model.py\")\n",
    "copyfile(src = \"../input/othermodules/viz.py\", dst = \"../working/viz.py\")\n",
    "copyfile(src = \"../input/othermodules/utils.py\", dst = \"../working/utils.py\")\n",
    "\n",
    "# Import all functions\n",
    "from conv_filter_visualization import *\n",
    "\n",
    "# Models\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = VGG16()\n",
    "\n",
    "# summarize the model \n",
    "model.summary()\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# layer_name = '' \n",
    "input_image = \"../input/plantvillage/plantvillage_split_dataset/test_images/Test_100.jpg\"\n",
    "\n",
    "#plt.imshow(input_image)\n",
    "\n",
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map Visualization (VGG16 Keras Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image with the required shape\n",
    "img = load_img(input_image, target_size=(224, 224))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()\n",
    "\n",
    "# load the VGG16 model\n",
    "model = VGG16()\n",
    "# redefine model to output right after the first hidden layer\n",
    "ixs = [2, 5, 9, 13, 17]\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "model = Model(inputs=model.inputs, outputs=outputs)\n",
    "# load the image with the required shape\n",
    "img = load_img(input_image, target_size=(224, 224))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot the output from each block\n",
    "square = 8\n",
    "for fmap in feature_maps:\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    ix = 1\n",
    "    plt.figure(figsize = (15,15))\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            pyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
    "            ix += 1\n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Visualization (InPhyT Model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import glob\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.Sequential([\n",
    "   # tf.keras.Input(shape=(150, 150, 3)),\n",
    "    #data_augmentation(inputs),\n",
    "#     tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "#     tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "#     tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    # Note the input shape is the desired size of the image 200x 200 with 3 bytes color\n",
    "    # The first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(224, 224, 3)), #, input_shape=(150, 150, 3)\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a dense layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 128 neuron in the fully-connected layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # 5 output neurons for 5 classes with the softmax activation\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics = ['categorical_accuracy'],optimizer='adam')\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# layer_name = '' \n",
    "input_image = \"../input/plantvillage/plantvillage_split_dataset/test_images/Test_100.jpg\" \n",
    "\n",
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 7, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray') \n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map Visualization (InPhyT Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.Sequential([\n",
    "   # tf.keras.Input(shape=(150, 150, 3)),\n",
    "    #data_augmentation(inputs),\n",
    "#     tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "#     tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "#     tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    # Note the input shape is the desired size of the image 200x 200 with 3 bytes color\n",
    "    # The first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(224, 224, 3)), #, input_shape=(150, 150, 3)\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a dense layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 128 neuron in the fully-connected layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # 5 output neurons for 5 classes with the softmax activation\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "ixs = [2,3,4,5,6,7,8, 9, 10]\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "model = Model(inputs=model.inputs, outputs=outputs)\n",
    "# load the image with the required shape\n",
    "img = load_img(input_image, target_size=(224, 224))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot the output from each block\n",
    "square = 5\n",
    "\n",
    "#fig, ax = subplots(figsize=(18, 2))\n",
    "#ax.imshow(random.rand(8, 90), interpolation='nearest'\n",
    "\n",
    "for fmap in feature_maps:\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    ix = 1\n",
    "    plt.figure(figsize = (15,15))\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            try:\n",
    "                pyplot.imshow(fmap[0, :, :, ix-1], cmap = 'gray')\n",
    "            except:\n",
    "                print(\"\", end = \"\\r\")\n",
    "            ix += 1\n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Visualization (DenseNet121 Keras Model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet121 (Functional)     (None, 7, 7, 1024)        7037504   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_11  (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 7,041,604\n",
      "Trainable params: 6,957,956\n",
      "Non-trainable params: 83,648\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5049ff5cea3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# retrieve weights from the second hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# normalize filter values to 0-1 so we can visualize them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Load the model \n",
    "model = tf.keras.Sequential([DenseNet121(input_shape=(224, 224, 3),\n",
    "                                         weights='imagenet',\n",
    "                                         include_top=False),\n",
    "                             L.GlobalAveragePooling2D(),\n",
    "                             L.Dense(4,activation='softmax')])\n",
    "        \n",
    "model.compile(loss='categorical_crossentropy', metrics = ['categorical_accuracy'], optimizer='adam')\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# layer_name = '' \n",
    "input_image = \"../input/plantvillage/plantvillage_split_dataset/test_images/Test_100.jpg\" \n",
    "\n",
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 7, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray') \n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map Visualization (DenseNet121 Keras Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \n",
    "model = tf.keras.Sequential([DenseNet121(input_shape=(224, 224, 3),\n",
    "                                         weights='imagenet',\n",
    "                                         include_top=False),\n",
    "                             L.GlobalAveragePooling2D(),\n",
    "                             L.Dense(4,activation='softmax')])\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "ixs = [1,2]\n",
    "outputs = [model.layers[i].output for i in ixs]\n",
    "model = Model(inputs=model.inputs, outputs=outputs)\n",
    "# load the image with the required shape\n",
    "img = load_img(input_image, target_size=(224, 224))\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "img = expand_dims(img, axis=0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img = preprocess_input(img)\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(img)\n",
    "# plot the output from each block\n",
    "square = 5\n",
    "\n",
    "#fig, ax = subplots(figsize=(18, 2))\n",
    "#ax.imshow(random.rand(8, 90), interpolation='nearest'\n",
    "\n",
    "for fmap in feature_maps:\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    ix = 1\n",
    "    plt.figure(figsize = (15,15))\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            pyplot.imshow(fmap[0, :, :, ix-1], cmap = 'gray')\n",
    "            ix += 1\n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
